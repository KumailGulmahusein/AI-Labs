#Pull data into Rstudio
winedata = read.csv('C:\\Users\\Kumail\\Desktop\\AI Labs\\Lab 2\\winedata.csv', sep=",")
#Separate class and values into separate variables
wineclass = winedata[,1]
winevalues = winedata[,-1]
#Build a training set of the first 100 values and test set the remaining values (101-178)
#set up a training set
wineclassTrain = wineclass[1:100]
winevaluesTrain = winevalues[1:100,]
#and testset
wineclassTest = wineclass[100:178]
winevaluesTest = winevalues[100:178,]
#Build decision tree with command rpart
install.packages("rpart")
library(rpart)
fit <- rpart(wineclassTrain~., method="class", data=winevaluesTrain)
#Plot decision tree
plot(fit, uniform=TRUE, main="Decision Tree for WineData3")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
#Test classifier on test set by calculating the predictions for each test case in our test set
treepred <-predict(fit, winevaluesTest, type = 'class')
#Compare to actual test class values to get accuracy
n = length(wineclassTest) #the number of test cases
ncorrect = sum(treepred==wineclassTest) #the number of correctly predicted
accuracy=ncorrect/n
print(accuracy)
#View results as confusion matrix
table_mat = table(wineclassTest, treepred)
print(table_mat)
#Effect of pruning
pfit<- prune(fit, cp=0.1)
plot(pfit, uniform=TRUE, main="Pruned Decision Tree for WineData3")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
#Install class library
library(class)
#Generate our predicted classes
knn3pred = knn(winevaluesTrain, winevaluesTest, wineclassTrain, k=3)
#Calculate accuracy as before
n = length(wineclassTest) #the number of test cases
ncorrect = sum(knn3pred==wineclassTest) #the number of correctly predicted
accuracy=ncorrect/n
print(accuracy)
#Pull data into Rstudio
seeddata = read.csv('C:\\Users\\Kumail\\Desktop\\AI Labs\\Lab 2\\seeds_dataset_class.csv', sep=",")
#Randomize data
seeds_rand=seeddata[sample(209,209),]
#Separate class and values into separate variables
seedclass = seeds_rand[,1]
seedvalues = seeds_rand[,-1]
#Build a training set of the first 125 values and test set the remaining values (125-209)
#set up a training set
seedclassTrain = seedclass[1:125]
seedvaluesTrain = seedvalues[1:125,]
#and test set
seedclassTest = seedclass[125:209]
seedvaluesTest = seedvalues[125:209,]
#Build decision tree with command r part
install.packages("rpart")
install.packages("rpart")
library(rpart)
fit <- rpart(seedclassTrain~., method="class", data=seedvaluesTrain)
#Plot decision tree
plot(fit, uniform=TRUE, main="Decision Tree for Seed Data")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
#Test classifier on test set by calculating the predictions for each test case in our test set
treepred <-predict(fit, seedvaluesTest, type = 'class')
#Compare to actual test class values to get accuracy
n = length(seedclassTest) #the number of test cases
ncorrect = sum(treepred==seedclassTest) #the number of correctly predicted
accuracy=ncorrect/n
print(accuracy)
#Prune tree to make data simpler
pfit<- prune(fit, cp=0.1)
plot(pfit, uniform=TRUE, main="Pruned Decision Tree for seed Data")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
printcp(pfit)
#Draw scatter plot of Groove length and area with color
plot(seeddata$GrooveLength, seeddata$Area, pch=16, col = seeddata$Class)
#Install class library
library(class)
#Assign desired k values to 'a'
a <- c(3,5,7,9)
KaccuracyValues <- vector(mode = "numeric")
for (i in a){
#Generate our predicted classes
knn3pred = knn(seedvaluesTrain, seedvaluesTest, seedclassTrain, k=i)
#Calculate accuracy as before
n = length(seedclassTest) #the number of test cases
ncorrect = sum(knn3pred==seedclassTest) #the number of correctly predicted
Kaccuracy=ncorrect/n
print(Kaccuracy)
#Print 'i' to compare against 'k' value
print(i)
#Store Kaccuracy Values
KaccuracyValues <- append(KaccuracyValues, Kaccuracy)
}
